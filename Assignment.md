# Практическое задание 2: Генерация имени функции по её телу

## Общая идея

Необходимо разработать решение на языке **Python**, которое **предсказывает имя функции на основе её тела и документации**.
Задача направлена на анализ исходного кода и применение современных предобученных моделей для работы с программным кодом (например, **CodeT5+** или **CodeBERT**).
Результатом станет система, которая получает код функции и генерирует подходящее имя, отражающее её назначение.

---

## 1. Подготовка данных

**Цель:** создать и обработать набор данных, содержащий тела функций и их реальные имена, используя открытые исходники из репозиториев.

**Основные шаги:**

* Использовать датасет [**CodeSearchNet**](https://huggingface.co/datasets/code-search-net/code_search_net) (часть для языка *Python*).
* Загрузить данные через библиотеку `datasets` из экосистемы **Hugging Face**.
  Пример: `load_dataset("code-search-net/code_search_net", "python", split="test", trust_remote_code=True)`
* Ограничить выборку (например, первыми **1000** примерами) для ускорения экспериментов.
* Для каждого примера извлечь:

  * **Имя функции** — из поля `func_name`.
  * **Документацию (docstring)** — из поля `func_documentation_string`.
  * **Полный текст функции** — из поля `whole_func_string`.
* С помощью библиотеки **tree-sitter** и пакета `tree-sitter-python` выполнить синтаксический разбор и извлечь:

  * тело функции **без комментариев и docstring**;
  * тело функции **с комментариями и docstring**;
  * само имя функции (без префиксов классов, только локальное имя).
* Добавить эти данные в итоговый набор и сохранить в формате `.csv` или `.json`:

  ```
  func_name, body_no_comments, body_with_comments, docstring
  ```
* Проверить корректность извлечения, сравнив результаты с исходными полями `func_name` и `func_documentation_string`.
* Привести несколько примеров извлечённых функций в итоговом отчёте.

---

## 2. Использование предобученных моделей

**Цель:** применить предобученные модели обработки кода для генерации имён функций.

**Выбор модели:**

* [**CodeT5+ (Salesforce/codet5p-220m)**](https://huggingface.co/Salesforce/codet5p-220m)
* [**CodeBERT (microsoft/codebert-base)**](https://huggingface.co/microsoft/codebert-base)
* (при желании можно протестировать другие модели из экосистемы Hugging Face)

---

### 2.1. Предсказание по коду без комментариев

* Подготовить входные данные в виде функций, где имя заменено на специальный токен (`<extra_id_0>`).
  Пример:

  ```python
  def <extra_id_0>(x):
      return [i + 1 for i in x]
  ```
* Передать такие тексты в модель для генерации.
* Получить предсказанные имена.
* Оценить качество результатов с помощью метрик:

  * **Exact Match (EM)** — доля полностью совпавших имён.
  * **ROUGE-1** — мера схожести по токенам.
* Для 1000 примеров тестовой выборки ожидаемые ориентиры:
  **EM ≈ 0.145**, **ROUGE-1 ≈ 0.38**.

---

### 2.2. Предсказание по коду с комментариями и документацией

* Использовать функции с включёнными docstring и комментариями как входные данные.
* Применить ту же модель и те же метрики оценки.
* Сравнить результаты с предыдущим экспериментом (по чистому коду).
  Для 1000 примеров ожидаемые ориентиры:
  **EM ≈ 0.209**, **ROUGE-1 ≈ 0.49**.

---

## 3. Оценка и анализ

**Цель:** провести сравнение качества моделей на разных типах входных данных и сделать выводы.

**Требуется:**

* Рассчитать значения `Exact Match` и `ROUGE` для обеих версий входных данных.
* Сравнить их в виде таблицы или краткой сводки.
* Привести несколько примеров:

  * успешных предсказаний (модель угадала корректно);
  * неудачных предсказаний (модель ошиблась или дала общее имя).
* Сделать краткий вывод: как наличие документации и комментариев влияет на точность модели.

---

## 4. Итоговые результаты

В отчёте должны присутствовать:

* описание цели и структуры проекта;
* краткий обзор подхода (CodeSearchNet + tree-sitter + CodeT5/CodeBERT);
* значения метрик EM и ROUGE;
* 2–3 примера предсказаний;
* выводы о влиянии дополнительного контекста (docstring, комментариев) на качество предсказания.

---

## 5. Формат и сдача

* Допустимые форматы:

  * **Jupyter Notebook (`.ipynb`)** с поясняющими блоками;
  * или модуль **Python (`main.py`)** с CLI-интерфейсом.
* Все зависимости должны быть установлены через:

  ```bash
  pip install -r requirements.txt
  ```

  (включая `transformers`, `datasets`, `evaluate`, `tree-sitter`, `torch`).
* Сдача проекта: репозиторий или ссылка на Google Colab.
* Отчёт оформляется в формате `.md` или `.pdf` (1–2 страницы).
* При необходимости возможно устное объяснение подхода.
* Максимум — **3 балла**:

  * 1 балл — корректная подготовка данных;
  * 1 балл — предсказание на коде без комментариев;
  * 1 балл — предсказание на коде с docstring и анализ результатов.
